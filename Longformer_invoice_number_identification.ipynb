{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image processing and OCR tools\n",
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments, DistilBertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from natsort import natsorted  # For naturally sorted lists (e.g., invoice1, invoice2, invoice10)\n",
    "\n",
    "# Converts a PDF file into a list of image objects (one per page)\n",
    "def pdf_to_image(pdf_file):\n",
    "    return convert_from_path(pdf_file)\n",
    "\n",
    "# Performs OCR (optical character recognition) on a single image file\n",
    "def image_to_text(file):\n",
    "    return image_to_string(file)\n",
    "\n",
    "# Processes a full PDF file: converts to images → performs OCR → combines the text from all pages\n",
    "def get_text_from_any_pdf(pdf_file):\n",
    "    images = pdf_to_image(pdf_file)\n",
    "    final_text = \"\"\n",
    "    for pg, img in enumerate(images):\n",
    "        final_text += image_to_text(img)\n",
    "    return final_text\n",
    "\n",
    "# List of PDF invoice files to be processed\n",
    "pdfs = [\"invoice1.pdf\", \"invoice2.pdf\", \"invoice3.pdf\"]\n",
    "\n",
    "# Load the Excel file containing ground truth invoice numbers\n",
    "invoice_numbers_path = \"invoices.xlsx\"\n",
    "df = pd.read_excel(invoice_numbers_path)\n",
    "\n",
    "# Extract invoice numbers from the Excel file\n",
    "invoice_numbers = []\n",
    "for index, row in df.iterrows():\n",
    "    invoice_number = row[1:].dropna().tolist()\n",
    "    invoice_number = [str(num) for num in invoice_number if num is not None]\n",
    "    invoice_numbers.append(invoice_number)\n",
    "\n",
    "\n",
    "# Extract text from all invoice PDFs using OCR\n",
    "texts = [get_text_from_any_pdf(pdf) for pdf in pdfs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bereinigung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizerFast, LongformerForTokenClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split PDFs, invoice number labels and extracted texts into training and test sets\n",
    "train_pdfs, test_pdfs, train_labels, test_labels, train_texts, test_texts = train_test_split(\n",
    "    pdfs, invoice_numbers, texts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load Longformer tokenizer and model for token classification (e.g., NER-style)\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\", add_prefix_space=True)\n",
    "model = LongformerForTokenClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=3)\n",
    "# Labels: 0 = O (outside), 1 = B (beginning of entity), 2 = I (inside of entity)\n",
    "\n",
    "# Convert a list of tokens and known labels (e.g., invoice numbers) into NER-style tags\n",
    "def label_tokens(tokens, labels):\n",
    "    ner_tags = [0] * len(tokens)  # Default to \"O\" (no entity)\n",
    "    for label in labels:\n",
    "        label_parts = label.split()\n",
    "        for i in range(len(tokens) - len(label_parts) + 1):\n",
    "            if tokens[i:i+len(label_parts)] == label_parts:\n",
    "                ner_tags[i] = 1  # \"B\" – beginning of the entity\n",
    "                for j in range(1, len(label_parts)):\n",
    "                    ner_tags[i + j] = 2  # \"I\" – inside the entity\n",
    "                break\n",
    "    return ner_tags\n",
    "\n",
    "# Split the token sequence into overlapping segments suitable for Longformer input length\n",
    "def split_into_segments(text, labels, max_length=2648, overlap=256):\n",
    "    tokens = text.split()\n",
    "    ner_tags = label_tokens(tokens, labels)\n",
    "    segments = []\n",
    "    for start_idx in range(0, len(tokens), max_length - overlap):\n",
    "        end_idx = min(start_idx + max_length, len(tokens))\n",
    "        segment_tokens = tokens[start_idx:end_idx]\n",
    "        segment_labels = ner_tags[start_idx:end_idx]\n",
    "        segments.append({\"tokens\": segment_tokens, \"ner_tags\": segment_labels})\n",
    "        if end_idx == len(tokens):\n",
    "            break\n",
    "    return segments\n",
    "\n",
    "# Create training segments by slicing text + labels into overlapping token windows\n",
    "train_data = []\n",
    "for text, labels in zip(train_texts, train_labels):\n",
    "    train_data.extend(split_into_segments(text, labels))\n",
    "\n",
    "# Do the same for the test data\n",
    "test_data = []\n",
    "for text, labels in zip(test_texts, test_labels):\n",
    "    test_data.extend(split_into_segments(text, labels))\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Tokenize the input tokens and align the NER labels accordingly\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=2648,\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100) \n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization and label alignment to training and test datasets\n",
    "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Create a DatasetDict for use in training/evaluation\n",
    "dataset = DatasetDict({\"train\": train_tokenized, \"test\": test_tokenized})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "wandb.init(project=\"Training Plots\")\n",
    "\n",
    "# Split training dataset into training (80%) and validation (20%) sets\n",
    "train_valid_split = train_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Combine train, validation, and test into a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_valid_split['train'],\n",
    "    'validation': train_valid_split['test'],\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Evaluation metrics for token classification (ignoring special tokens)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored tokens (-100) from both predictions and labels\n",
    "    true_labels = [[label for label in label_group if label != -100] for label_group in labels]\n",
    "    true_predictions = [\n",
    "        [pred for (pred, label) in zip(pred_group, label_group) if label != -100]\n",
    "        for pred_group, label_group in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Flatten for metric calculation\n",
    "    y_true = [item for sublist in true_labels for item in sublist]\n",
    "    y_pred = [item for sublist in true_predictions for item in sublist]\n",
    "\n",
    "    results = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": results[0],\n",
    "        \"recall\": results[1],\n",
    "        \"f1\": results[2],\n",
    "    }\n",
    "\n",
    "# Tokenize inputs and align token-level labels with word-level annotations\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=2648,\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens will be ignored\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  # First sub-token of word gets the label\n",
    "            else:\n",
    "                label_ids.append(-100)  # Remaining sub-tokens are ignored\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize and align labels for each split\n",
    "train_tokenized = dataset[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "validation_tokenized = dataset[\"validation\"].map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized = dataset[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Load Longformer model with 3 labels for token classification (e.g., O, B, I)\n",
    "model = LongformerForTokenClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Define training arguments for the Hugging Face Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # Directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
    "    logging_dir='./logs',                   # Directory for log files\n",
    "    logging_steps=10,                       # Log every 10 steps\n",
    "    num_train_epochs=3,                     # Number of training epochs\n",
    "    per_device_train_batch_size=1,          # Batch size per device during training\n",
    "    per_device_eval_batch_size=1,           # Batch size for evaluation\n",
    "    save_steps=10_000,                      # Save model every 10,000 steps (not likely in small datasets)\n",
    "    save_total_limit=2,                     # Keep only the 2 most recent checkpoints\n",
    "    learning_rate=5e-5,                     # Learning rate for optimizer\n",
    "    report_to=\"wandb\"                       # Enable Weights & Biases logging\n",
    ")\n",
    "\n",
    "# Initialize the Hugging Face Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=validation_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Run evaluation on the test set using the trained model\n",
    "results = trainer.predict(test_tokenized)\n",
    "\n",
    "# Extract raw model predictions and true labels\n",
    "predictions = results.predictions\n",
    "labels = results.label_ids\n",
    "\n",
    "# Convert logits to predicted class indices\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove special tokens (-100) from labels and predictions\n",
    "true_labels = [\n",
    "    [label for label in label_group if label != -100]\n",
    "    for label_group in labels\n",
    "]\n",
    "true_predictions = [\n",
    "    [pred for (pred, label) in zip(pred_group, label_group) if label != -100]\n",
    "    for pred_group, label_group in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "# Flatten the lists for metric calculations\n",
    "true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
    "true_predictions_flat = [item for sublist in true_predictions for item in sublist]\n",
    "\n",
    "# Function to compute classification metrics and confusion matrix\n",
    "def compute_metrics_per_class(true_labels_flat, true_predictions_flat):\n",
    "    report = classification_report(\n",
    "        true_labels_flat,\n",
    "        true_predictions_flat,\n",
    "        target_names=[\"O\", \"I-invoice_number\", \"B-invoice_number\"]\n",
    "    )\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(true_labels_flat, true_predictions_flat)\n",
    "    return conf_matrix, report\n",
    "\n",
    "# Compute metrics and confusion matrix\n",
    "conf_matrix, report = compute_metrics_per_class(true_labels_flat, true_predictions_flat)\n",
    "\n",
    "# Print basic metrics\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(true_labels_flat, true_predictions_flat)}\")\n",
    "print(f\"Precision: {precision_recall_fscore_support(true_labels_flat, true_predictions_flat, average='weighted')[0]}\")\n",
    "print(f\"Recall: {precision_recall_fscore_support(true_labels_flat, true_predictions_flat, average='weighted')[1]}\")\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(true_labels_flat, true_predictions_flat, average='weighted')[2]}\")\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[\"O\", \"I-invoice_number\", \"B-invoice_number\"],\n",
    "    yticklabels=[\"O\", \"I-invoice_number\", \"B-invoice_number\"]\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Identify misclassified tokens and their surrounding context\n",
    "misclassifications = []\n",
    "for i, (pred, true) in enumerate(zip(true_predictions, true_labels)):\n",
    "    for j, (p, t) in enumerate(zip(pred, true)):\n",
    "        if p != t:\n",
    "            # Extract surrounding tokens (±5) for context\n",
    "            context_tokens = dataset[\"test\"][i]['tokens'][max(0, j-5):min(len(dataset[\"test\"][i]['tokens']), j+6)]\n",
    "            context = \" \".join(context_tokens)\n",
    "            misclassifications.append((i, j, p, t, context))\n",
    "\n",
    "# Print misclassified tokens with context\n",
    "for doc_idx, token_idx, pred, true, context in misclassifications:\n",
    "    print(\n",
    "        f\"Document {doc_idx}, Token {token_idx}: \"\n",
    "        f\"Predicted {pred}, Actual {true}, \"\n",
    "        f\"Context: {test_tokenized['tokens'][doc_idx][token_idx]}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
